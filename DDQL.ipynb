{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seifmostafa73/Cartpole-DDQL/blob/main/DDQL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7b8ac71",
      "metadata": {
        "scrolled": false,
        "id": "c7b8ac71",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a66d5a7d-ec08-47e1-f785-8161913b141b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym==0.26.2 in /usr/local/lib/python3.8/dist-packages (0.26.2)\n",
            "Requirement already satisfied: tensorflow==2.11.0 in /usr/local/lib/python3.8/dist-packages (2.11.0)\n",
            "Requirement already satisfied: keras==2.11.0 in /usr/local/lib/python3.8/dist-packages (2.11.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym==0.26.2) (0.0.8)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym==0.26.2) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym==0.26.2) (4.13.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym==0.26.2) (1.5.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (1.6.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (2.1.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (3.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (2.11.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (0.28.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (0.2.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (3.19.6)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (14.0.6)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (1.50.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (22.12.6)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (1.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (57.4.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (1.14.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (4.1.1)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (2.11.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow==2.11.0) (3.3.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow==2.11.0) (0.38.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym==0.26.2) (3.10.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.4.6)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.14.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.6.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (5.2.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11.0) (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow==2.11.0) (3.0.9)\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3021, in _dep_map\n",
            "    return self.__dep_map\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2815, in __getattr__\n",
            "    raise AttributeError(attr)\n",
            "AttributeError: _DistInfoDistribution__dep_map\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/base_command.py\", line 180, in _main\n",
            "    status = self.run(options, args)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/req_command.py\", line 199, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/commands/install.py\", line 385, in run\n",
            "    conflicts = self._determine_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/commands/install.py\", line 515, in _determine_conflicts\n",
            "    return check_install_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/operations/check.py\", line 103, in check_install_conflicts\n",
            "    package_set, _ = create_package_set_from_installed()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/operations/check.py\", line 45, in create_package_set_from_installed\n",
            "    package_set[name] = PackageDetails(dist.version, dist.requires())\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2736, in requires\n",
            "    dm = self._dep_map\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3023, in _dep_map\n",
            "    self.__dep_map = self._compute_dependencies()\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3033, in _compute_dependencies\n",
            "    reqs.extend(parse_requirements(req))\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3094, in parse_requirements\n",
            "    yield Requirement(line)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3101, in __init__\n",
            "    super(Requirement, self).__init__(requirement_string)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/packaging/requirements.py\", line 113, in __init__\n",
            "    req = REQUIREMENT.parseString(requirement_string)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing.py\", line 1943, in parseString\n",
            "    loc, tokens = self._parse(instring, 0)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing.py\", line 4069, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing.py\", line 4254, in parseImpl\n",
            "    ret = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing.py\", line 4052, in parseImpl\n",
            "    loc, resultlist = self.exprs[0]._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing.py\", line 4069, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing.py\", line 4849, in parseImpl\n",
            "    loc, tokens = self.expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing.py\", line 4254, in parseImpl\n",
            "    ret = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing.py\", line 4069, in parseImpl\n",
            "    loc, exprtokens = e._parse(instring, loc, doActions)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing.py\", line 4462, in parseImpl\n",
            "    return self.expr._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing.py\", line 4052, in parseImpl\n",
            "    loc, resultlist = self.exprs[0]._parse(instring, loc, doActions, callPreParse=False)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing.py\", line 1683, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing.py\", line 4132, in parseImpl\n",
            "    loc2 = e.tryParse(instring, loc)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing.py\", line 1736, in tryParse\n",
            "    return self._parse(instring, loc, doActions=False)[0]\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing.py\", line 1687, in _parseNoCache\n",
            "    loc, tokens = self.parseImpl(instring, preloc, doActions)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/pyparsing.py\", line 3340, in parseImpl\n",
            "    result = self.re_match(instring, loc)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/main.py\", line 71, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/base_command.py\", line 104, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/base_command.py\", line 213, in _main\n",
            "    logger.debug(\"Exception information:\", exc_info=True)\n",
            "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1434, in debug\n",
            "    self._log(DEBUG, msg, args, **kwargs)\n",
            "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1589, in _log\n",
            "    self.handle(record)\n",
            "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1599, in handle\n",
            "    self.callHandlers(record)\n",
            "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1661, in callHandlers\n",
            "    hdlr.handle(record)\n",
            "  File \"/usr/lib/python3.8/logging/__init__.py\", line 954, in handle\n",
            "    self.emit(record)\n",
            "  File \"/usr/lib/python3.8/logging/handlers.py\", line 71, in emit\n",
            "    logging.FileHandler.emit(self, record)\n",
            "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1187, in emit\n",
            "    StreamHandler.emit(self, record)\n",
            "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1085, in emit\n",
            "    msg = self.format(record)\n",
            "  File \"/usr/lib/python3.8/logging/__init__.py\", line 929, in format\n",
            "    return fmt.format(record)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/utils/logging.py\", line 130, in format\n",
            "    formatted = super().format(record)\n",
            "  File \"/usr/lib/python3.8/logging/__init__.py\", line 676, in format\n",
            "    record.exc_text = self.formatException(record.exc_info)\n",
            "  File \"/usr/lib/python3.8/logging/__init__.py\", line 626, in formatException\n",
            "    traceback.print_exception(ei[0], ei[1], tb, None, sio)\n",
            "  File \"/usr/lib/python3.8/traceback.py\", line 103, in print_exception\n",
            "    for line in TracebackException(\n",
            "  File \"/usr/lib/python3.8/traceback.py\", line 508, in __init__\n",
            "    self.stack = StackSummary.extract(\n",
            "  File \"/usr/lib/python3.8/traceback.py\", line 366, in extract\n",
            "    f.line\n",
            "  File \"/usr/lib/python3.8/traceback.py\", line 288, in line\n",
            "    self._line = linecache.getline(self.filename, self.lineno).strip()\n",
            "  File \"/usr/lib/python3.8/linecache.py\", line 16, in getline\n",
            "    lines = getlines(filename, module_globals)\n",
            "  File \"/usr/lib/python3.8/linecache.py\", line 47, in getlines\n",
            "    return updatecache(filename, module_globals)\n",
            "  File \"/usr/lib/python3.8/linecache.py\", line 136, in updatecache\n",
            "    with tokenize.open(fullname) as fp:\n",
            "  File \"/usr/lib/python3.8/tokenize.py\", line 394, in open\n",
            "    encoding, lines = detect_encoding(buffer.readline)\n",
            "  File \"/usr/lib/python3.8/tokenize.py\", line 363, in detect_encoding\n",
            "    first = read_or_stop()\n",
            "  File \"/usr/lib/python3.8/tokenize.py\", line 321, in read_or_stop\n",
            "    return readline()\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "%pip install gym==0.26.2 tensorflow==2.11.0 keras==2.11.0 atari-py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7OZpdjk3qdI",
        "outputId": "6a678d7c-e832-44c1-c4d7-a08074d6f056"
      },
      "id": "m7OZpdjk3qdI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9a05433",
      "metadata": {
        "id": "e9a05433"
      },
      "source": [
        "# ***DQL Sudo Code summary***\n",
        "1) Initialize replay memory capacity.\n",
        "2) Initialize the policy network with random weights.\n",
        "3) Clone the policy network, and call it the target network.\n",
        "4) For each episode:\n",
        "5) Initialize the starting state.\n",
        "6) For each time step:\n",
        "7) Select an action.\n",
        "8) Via exploration or exploitation\n",
        "9) Execute selected action in an emulator.\n",
        "10) Observe reward and next state.\n",
        "11) Store experience in replay memory.\n",
        "12) Sample random batch from replay memory.\n",
        "13) Preprocess states from batch.\n",
        "14) Pass batch of preprocessed states to policy network.\n",
        "15) Calculate loss between output Q-values and target Q-values.\n",
        "16) Requires a pass to the target network for the next state\n",
        "17) Gradient descent updates weights in the policy network to minimize loss.\n",
        "18) After  time steps, weights in the target network are updated to the weights in the policy network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14c60405",
      "metadata": {
        "scrolled": false,
        "id": "14c60405"
      },
      "outputs": [],
      "source": [
        "#includes\n",
        "from collections import deque \n",
        "import gym\n",
        "import math\n",
        "import datetime\n",
        "import codecs, json \n",
        "import time\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "from collections import namedtuple\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import gc\n",
        "from tensorflow.keras import backend as k\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from IPython import display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cc2acc9",
      "metadata": {
        "id": "6cc2acc9"
      },
      "outputs": [],
      "source": [
        "#hyper paramters\n",
        "\n",
        "EPISODES = 1001        #this refers to the number of episodes we want or agent to play\n",
        "MAX_STEPS = 1000           #this refers to the Maxnumber of steps at each episode , if exceeded the episode will end immeditely  \n",
        "BATCH_SIZE = 128         #replay memory batch size (this will be used to traing a single step , we will randomize them and then take the highest Q value among all of them as our result Q value for the current state)\n",
        "DISCOUNT_RATE  = 0.95   #ùõæ : rate of decay of rewards\n",
        "exploration_rate = 1     #Œµ : prob. of our agent exploring the environment instead of exploiting it\n",
        "MAX_EXPLORATION_RATE = 1\n",
        "MIN_EXPLORATION_RATE = 0.01\n",
        "EXPLOARTION_DECAY = 0.0001\n",
        "TARGET_UPDATE = 10      #number of episodes to update the target NN with policy NN weights\n",
        "MEMORY_SIZE = 2000    #size of replay memory\n",
        "LEARNING_RATE  = 0.001    #Œ± : rate of updating Q value  \n",
        "\n",
        "MODEL_NAME = 'DQL_MODEL'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7499c54",
      "metadata": {
        "scrolled": true,
        "id": "b7499c54"
      },
      "outputs": [],
      "source": [
        "# defining Expericne Class \"custom Data strucure\"\n",
        "Experience = namedtuple('Experience',['state','action','new_state','reward','done'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da61706c",
      "metadata": {
        "id": "da61706c"
      },
      "outputs": [],
      "source": [
        "#ReplayMemory Class\n",
        "class ReplayMemory():\n",
        "    def __init__(self,mem_capacity):\n",
        "        self.memory_capacity = mem_capacity # max capacity of replay memory before overriding from beggining\n",
        "        self.memory = [] # will store number of experiences\n",
        "        self.count = 0 # keeps track of added experiences\n",
        "    \n",
        "    def push(self,new_experience):\n",
        "        #simple if memory is still not full append at end\n",
        "        if len(self.memory) < self.memory_capacity:\n",
        "            self.memory.append(new_experience)\n",
        "        #if not then replace first element\n",
        "        else:\n",
        "            self.memory[self.count % self.memory_capacity] = new_experience\n",
        "        self.count +=1\n",
        "    \n",
        "    def sample(self):\n",
        "        #when we sample we want to return a random set of experinces of size (batch size) from replay memory to train our NN with\n",
        "        return random.sample(self.memory,BATCH_SIZE)\n",
        "    \n",
        "    def can_sample(self,batch_size):\n",
        "        #you can only sample a batch if there exist enough experinces\n",
        "        return len(self.memory) >= batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1da70694",
      "metadata": {
        "id": "1da70694"
      },
      "outputs": [],
      "source": [
        "#env manager class\n",
        "class Environment():\n",
        "    def __init__(self,num_actions):\n",
        "        self.env = gym.make('CartPole-v0')\n",
        "        self.reset()\n",
        "        self.done = False # set to true if last step endded the episode\n",
        "        self.current_step = 0\n",
        "        self.num_actions = self.num_actions_available()\n",
        "        self.exploration_rate = exploration_rate\n",
        "        \n",
        "    def select_action(self,policy_net):\n",
        "        #update epsilon\n",
        "        if random.random() <= self.exploration_rate : #explore\n",
        "            self.exploration_rate = MIN_EXPLORATION_RATE + (MAX_EXPLORATION_RATE - MIN_EXPLORATION_RATE) * np.exp(-EXPLOARTION_DECAY * self.current_step)\n",
        "            return random.randrange(self.num_actions)\n",
        "        else: #exploit\n",
        "            self.exploration_rate = MIN_EXPLORATION_RATE + (MAX_EXPLORATION_RATE - MIN_EXPLORATION_RATE) * np.exp(-EXPLOARTION_DECAY * self.current_step)\n",
        "            output = policy_net.predict(self.current_state)\n",
        "            return (np.argmax(output))\n",
        "    \n",
        "    def take_action(self, action):\n",
        "        self.current_step +=1\n",
        "        old_state = self.current_state\n",
        "        new_state,reward,self.done,_,_ = self.env.step(action)\n",
        "        reward = reward if not self.done else -10\n",
        "        self.current_state = new_state\n",
        "        return Experience(old_state,action,new_state,reward,self.done)\n",
        "    \n",
        "    \"\"\"\n",
        "    note that we will represent a single state in the environment\n",
        "    as the current agent state stored in self.env porperies \n",
        "    \"\"\"\n",
        "    \n",
        "    #some wrapper functions \n",
        "    def reset(self):\n",
        "        self.current_state = self.env.reset()[0]\n",
        "\n",
        "    def close(self):\n",
        "        print(\"Closing\")\n",
        "        self.env.close()\n",
        "\n",
        "    def render(self):\n",
        "        return np.array(self.env.render()) #this renders screen and returns numpy array version of the rendered screen\n",
        "    \n",
        "    def num_actions_available(self):\n",
        "        return self.env.action_space.n\n",
        "    \n",
        "    def just_starting(self):\n",
        "        return self.current_screen is None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b697ab4",
      "metadata": {
        "id": "2b697ab4"
      },
      "outputs": [],
      "source": [
        "#DQN(deep q network) class\n",
        "class DQN():\n",
        "    def __init__(self,inputshape,outputshape):\n",
        "        self.input_shape = inputshape\n",
        "        self.model = Sequential()\n",
        "        self.model.add(Dense(24, input_shape=inputshape, activation='relu')) # 1st hidden layer; states as input\n",
        "        self.model.add(Dense(24, activation='relu')) # 2nd hidden layer\n",
        "        self.model.add(Dense(outputshape, activation='linear')) # 2 actions, so 2 output neurons: 0 and 1 (L/R)\n",
        "        self.model.compile(optimizer=Adam(learning_rate=LEARNING_RATE),loss='mse')\n",
        "        self.model.summary()\n",
        "            \n",
        "    def predict(self,state):\n",
        "        state = np.asarray(state).reshape(1,self.input_shape[0])\n",
        "        return self.model.predict(x=state,verbose = 0)\n",
        "\n",
        "class ClearMemory(Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        gc.collect()\n",
        "        k.clear_session()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78bbb4be",
      "metadata": {
        "id": "78bbb4be"
      },
      "outputs": [],
      "source": [
        "replay_memory = ReplayMemory(MEMORY_SIZE) \n",
        "rewards_buffer = np.zeros(EPISODES)\n",
        "\n",
        "def DQL(envir,policy_net,target_net):\n",
        "    target_net.model.set_weights(policy_net.model.get_weights())\n",
        "    target_step = 0\n",
        "\n",
        "    for episode in range(EPISODES):\n",
        "        envir.reset()\n",
        "        for step in range(MAX_STEPS):\n",
        "            current_action = envir.select_action(policy_net)\n",
        "            new_experience = envir.take_action(current_action)\n",
        "            \n",
        "            replay_memory.push(new_experience)\n",
        "            rewards_buffer[episode] +=new_experience.reward\n",
        "            \n",
        "            if new_experience.done == True:\n",
        "                break;\n",
        "            if replay_memory.can_sample(BATCH_SIZE):\n",
        "                retrieved_experiences = replay_memory.sample()\n",
        "                \n",
        "                current_states = np.array([retrieved_exp.state for retrieved_exp in retrieved_experiences])\n",
        "                new_states = np.array([retrieved_exp.new_state for retrieved_exp in retrieved_experiences])\n",
        "                \n",
        "                current_outputs = policy_net.model.predict(current_states,verbose=0)\n",
        "                next_outputs = target_net.model.predict(new_states,verbose=0)\n",
        "                \n",
        "                optimal_current_outputs =[]\n",
        "                \n",
        "                for i in range(BATCH_SIZE):\n",
        "                    max_next_output = np.max(next_outputs[i])    \n",
        "                    reward = retrieved_experiences[i].reward + DISCOUNT_RATE * max_next_output * (1-retrieved_experiences[i].done)\n",
        "                    optimal_output = np.array(current_outputs[i])\n",
        "                    optimal_output[retrieved_experiences[i].action] = reward\n",
        "                    optimal_current_outputs.append(optimal_output)\n",
        "                \n",
        "                policy_net.model.fit(current_states,np.array(optimal_current_outputs),batch_size=BATCH_SIZE,callbacks=ClearMemory(),verbose=0,shuffle=False)\n",
        "       \n",
        "        target_step+=1\n",
        "        if target_step >= TARGET_UPDATE:\n",
        "            target_step = 0\n",
        "            target_net.model.set_weights(policy_net.model.get_weights())\n",
        "        \n",
        "        avg_reward = np.mean(rewards_buffer[episode])\n",
        "        print(\"Current Episode :\",episode,\" Avg Rew: \",avg_reward)\n",
        "        print(envir.exploration_rate)\n",
        "        \n",
        "        if episode % 50 ==0:\n",
        "            print(\"Saved Policy model\")\n",
        "            policy_net.model.save_weights('/content/drive/MyDrive/Colab Notebooks/DQL Data/POLICY_NET_3.h5')\n",
        "            json.dump(rewards_buffer.tolist(), codecs.open('/content/drive/MyDrive/Colab Notebooks/DQL Data/output.json', 'w', encoding='utf-8'), separators=(',', ':'), sort_keys=True,indent=4) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d379f459",
      "metadata": {
        "scrolled": true,
        "id": "d379f459"
      },
      "outputs": [],
      "source": [
        "TRAING = False\n",
        "envir = Environment(2)\n",
        "policy_net = DQN(envir.env.observation_space.shape,envir.num_actions)\n",
        "target_net = DQN(envir.env.observation_space.shape,envir.num_actions)\n",
        "\n",
        "if TRAING:\n",
        "    #policy_net.model.load_weights('POLICY_NET_2.h5')\n",
        "    DQL(envir,policy_net,target_net)\n",
        "else:\n",
        "    policy_net.model.load_weights('/content/drive/MyDrive/Colab Notebooks/DQL Data/POLICY_NET_3_100STEPS.h5')\n",
        "    env = gym.make('CartPole-v0',render_mode=\"human\")\n",
        "    observation = env.reset()[0]\n",
        "    reward =0\n",
        "    step=0\n",
        "    while True:\n",
        "        env.render() \n",
        "        action = np.argmax(policy_net.predict(observation))\n",
        "        observation, r, done, info,_ = env.step(action)\n",
        "        step+=1\n",
        "        reward+=r\n",
        "        if done: \n",
        "            print('Episode finished after {} timesteps, total rewards {}'.format(step+1,np.cumsum(reward)))\n",
        "            observation = env.reset()[0]\n",
        "            reward =0\n",
        "            step=0\n",
        "envir.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UtfzpxKVwpwQ"
      },
      "id": "UtfzpxKVwpwQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z8kiOFjvp_tl"
      },
      "id": "Z8kiOFjvp_tl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ecfbc35",
      "metadata": {
        "scrolled": true,
        "id": "2ecfbc35"
      },
      "outputs": [],
      "source": [
        "envir.close()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}